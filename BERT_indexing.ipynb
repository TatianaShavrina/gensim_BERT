{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_indexing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNm6kk8gi48UnV0TAhxYfES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TatianaShavrina/gensim_BERT/blob/master/BERT_indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rSzCmSl5kYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ac1b5894-3947-4779-ced4-babd88c16eb6"
      },
      "source": [
        "!pip install pytorch_pretrained_bert nmslib"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: nmslib in /usr/local/lib/python3.6/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.1+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from nmslib) (2.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.9->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNvky5Zm7HUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16f993f8-b291-4b6b-ea8c-7172b709a3c2"
      },
      "source": [
        "! pip install rusenttokenize"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rusenttokenize in /usr/local/lib/python3.6/dist-packages (0.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVHyZkip6vCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re, json\n",
        "import json\n",
        "import nmslib\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "#import tensorflow_hub as hub\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from joblib import dump, load\n",
        "from string import punctuation\n",
        "from operator import itemgetter\n",
        "from functools import wraps\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rusenttokenize import ru_sent_tokenize"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOOmiXxl7CaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def singleton(cls):\n",
        "    instance = None\n",
        "    @wraps(cls)\n",
        "    def inner(*args, **kwargs):\n",
        "        nonlocal instance\n",
        "        if instance is None:\n",
        "            instance = cls(*args, **kwargs)\n",
        "        return instance\n",
        "    return inner"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW54k57v6s2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertEmbedder(object):\n",
        "    \"\"\"\n",
        "    Embedding Wrapper on Bert Multilingual Cased\n",
        "    \"\"\"\n",
        "    def __init__(self, path=''):\n",
        "        # use self.model_file with a path instead of 'bert-base-uncased' if you have a custom pretrained model\n",
        "        self.model_file = 'bert-base-uncased'#os.path.join(path, \"bert-base-multilingual-cased.tar.gz\")\n",
        "        self.vocab_file = 'bert-base-uncased'#os.path.join(path, \"data_bert-base-multilingual-cased-vocab.txt\")\n",
        "        self.model = self.bert_model()\n",
        "        self.tokenizer = self.bert_tokenizer()\n",
        "        self.embedding_matrix = self.get_bert_embed_matrix()\n",
        "\n",
        "    @singleton\n",
        "    def bert_model(self):\n",
        "        model = BertModel.from_pretrained(self.model_file).eval()\n",
        "        return model\n",
        "\n",
        "    @singleton\n",
        "    def bert_tokenizer(self):\n",
        "        tokenizer = BertTokenizer.from_pretrained(self.vocab_file, do_lower_case=False) \n",
        "        return tokenizer\n",
        "\n",
        "    @singleton\n",
        "    def get_bert_embed_matrix(self):\n",
        "        bert_embeddings = list(self.model.children())[0]\n",
        "        bert_word_embeddings = list(bert_embeddings.children())[0]\n",
        "        matrix = bert_word_embeddings.weight.data.numpy()\n",
        "        return matrix\n",
        "\n",
        "    def sentence_embedding(self, text):\n",
        "        token_list = self.tokenizer.tokenize(\"[CLS] \" + text + \" [SEP]\")\n",
        "        segments_ids, indexed_tokens = [1] * len(token_list), self.tokenizer.convert_tokens_to_ids(token_list)\n",
        "        segments_tensors, tokens_tensor = torch.tensor([segments_ids]), torch.tensor([indexed_tokens])\n",
        "        with torch.no_grad():\n",
        "            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
        "        sent_embedding = torch.mean(encoded_layers[11], 1)\n",
        "        return sent_embedding\n",
        "    \n",
        "    def sentences_embedding(self, text_list):\n",
        "        embeddings = []\n",
        "        for text in tqdm(text_list):\n",
        "            token_list = self.tokenizer.tokenize(\"[CLS] \" + text + \" [SEP]\")\n",
        "            segments_ids, indexed_tokens = [1] * len(token_list), self.tokenizer.convert_tokens_to_ids(token_list)\n",
        "            segments_tensors, tokens_tensor = torch.tensor([segments_ids]), torch.tensor([indexed_tokens])\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
        "            sent_embedding = torch.mean(encoded_layers[11], 1)\n",
        "            embeddings.append(sent_embedding)\n",
        "        return embeddings\n",
        "\n",
        "    def token_embedding(self, token_list):\n",
        "        token_embedding = []\n",
        "        for token in token_list:\n",
        "            ontoken = self.tokenizer.tokenize(token)\n",
        "            segments_ids, indexed_tokens = [1] * len(ontoken), self.tokenizer.convert_tokens_to_ids(ontoken)\n",
        "            segments_tensors, tokens_tensor = torch.tensor([segments_ids]), torch.tensor([indexed_tokens])\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
        "            ontoken_embeddings = []\n",
        "            for subtoken_i in range(len(ontoken)):\n",
        "                hidden_layers = []\n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    vector = encoded_layers[layer_i][0][subtoken_i]\n",
        "                    hidden_layers.append(vector)\n",
        "                ontoken_embeddings.append(hidden_layers)\n",
        "            cat_last_4_layers = [torch.cat((layer[-4:]), 0) for layer in ontoken_embeddings]\n",
        "            token_embedding.append(cat_last_4_layers)\n",
        "        token_embedding = torch.stack(token_embedding[0], 0) if len(token_embedding) > 1 else token_embedding[0][0]\n",
        "        return token_embedding\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyxnufk-_keu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b191bd1-1eae-4c56-ab82-0b259ba35584"
      },
      "source": [
        "class BertIndexer():\n",
        "    def __init__(self, bert_model=BertEmbedder()):\n",
        "        self.model = bert_model\n",
        "        self.space_type = 'cosinesimil'\n",
        "        self.method_name = 'hnsw'\n",
        "        self.index = nmslib.init(space  = space_type,\n",
        "                    method = method_name,\n",
        "                    data_type = nmslib.DataType.DENSE_VECTOR,\n",
        "                    dtype = nmslib.DistType.FLOAT)\n",
        "        self.index_params = {'NN' : 15}\n",
        "        self.index_is_loaded = False\n",
        "        self.data = []\n",
        "\n",
        "        class IndexError(Exception):\n",
        "            \"\"\"Base class for other exceptions\"\"\"\n",
        "            pass\n",
        "        \n",
        "    def load_index(self, index_path, data):\n",
        "        self.index.loadIndex( index_path, load_data=True)\n",
        "        self.index_is_loaded = True\n",
        "        self.data = data\n",
        "\n",
        "    def create_index(self, index_path, data):\n",
        "        names_sparse_matrix = self.make_data_embeddings(data)\n",
        "        self.index.addDataPointBatch(data = names_sparse_matrix)\n",
        "        self.index.createIndex( print_progress=True)\n",
        "        self.index.saveIndex(index_path, save_data=True)\n",
        "        self.index_is_loaded = True\n",
        "        self.data = data\n",
        "    \n",
        "    def create_embedding(self, text):\n",
        "        return self.model.sentence_embedding(text).numpy()\n",
        "        \n",
        "    def make_data_embeddings(self, data):\n",
        "        names_sparse_matrix = []\n",
        "        for i in tqdm(range(len(data))):\n",
        "            try:\n",
        "                names_sparse_matrix.append(self.model.sentence_embedding(data[i])[0].numpy())\n",
        "            except:\n",
        "                names_sparse_matrix.append(np.zeros(768))\n",
        "        return names_sparse_matrix\n",
        "\n",
        "    def return_closest(self, text, k=2, num_threads=2):\n",
        "        if self.index_is_loaded:\n",
        "            r = self.model.sentence_embedding(text).numpy()\n",
        "            near_neighbors = self.index.knnQueryBatch(queries=[r], k=k, num_threads=num_threads)\n",
        "            return [self.data[i] for i in near_neighbors[0][0]]\n",
        "        else: raise IndexError(\"Index is not yet created or loaded\")\n",
        "\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW_KRFUcFRIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexer = BertIndexer()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFWD5AUFFRCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ['мама мыла раму','папа мыл раму', 'Маша мыла раму', 'Россия - наше Отечество','Смерть неизбежна']"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSoZimFCFVDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04ca2ac9-523b-4ecf-b385-0c0ae94be215"
      },
      "source": [
        "indexer.create_index('test_index', data)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  8.96it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhZnwdTOFU-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac2c4660-d1ae-449e-ba21-493651a71d8a"
      },
      "source": [
        "indexer.return_closest('Россия', k=2)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Россия - наше Отечество', 'Смерть неизбежна']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jloLZdgiFplT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexer.load_index('test_index', data)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G0U5wfqF0d1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62cfa1f8-91e2-4923-d23f-7efc040bf93a"
      },
      "source": [
        "indexer.return_closest('Россия', k=1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Россия - наше Отечество']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}